{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.layers import Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "import os\n",
    "import pandas as pd\n",
    "from dataset_utilits import load_imagesPath_ages_sex\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import cv2\n",
    "from sklearn.decomposition import PCA\n",
    "from operator import itemgetter\n",
    "from inception_utilits import conv2d_bn, stem_block, inception_a_block, inception_b_block, inception_c_block, reduction_a_block, reduction_b_block\n",
    "from shapely.geometry import Polygon\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_truth = pd.read_csv('./data.nosync/train/train.csv')\n",
    "validation_truth = pd.read_csv('./data.nosync/validation/Validation Dataset.csv')\n",
    "test_truth = pd.read_excel('./data.nosync/test/Bone age ground truth.xlsx')\n",
    "IMG_SHAPE = (299, 299, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prendo le image_names da  /Users/pietroferrazzi/Desktop/uni/human_data/project/HDA-bone-age-prediction/data.nosync/train/images\n",
      "prendo le image_names da  /Users/pietroferrazzi/Desktop/uni/human_data/project/HDA-bone-age-prediction/data.nosync/validation/images\n",
      "prendo le image_names da  /Users/pietroferrazzi/Desktop/uni/human_data/project/HDA-bone-age-prediction/data.nosync/test/images\n"
     ]
    }
   ],
   "source": [
    "path_data_dir = 'data.nosync'\n",
    "full_path = os.path.join(os.getcwd(), path_data_dir)\n",
    "X_train, age_train, sex_train, X_validation, age_validation, sex_validation, X_test, age_test, sex_test = load_imagesPath_ages_sex(full_path, train_truth, validation_truth, test_truth, 500)\n",
    "\n",
    "age_train = [x / 200. for x in age_train]\n",
    "age_validation = [x / 200. for x in age_validation]\n",
    "age_train_df = pd.DataFrame(age_train, columns=['age'])\n",
    "age_validation_df = pd.DataFrame(age_validation, columns=['age'])\n",
    "age_train_df = age_train_df.astype('float32')\n",
    "age_validation_df = age_validation_df.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: 200\n",
      "X train shape: 500\n",
      "X val shape: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", len(X_test))\n",
    "print(\"X train shape:\", len(X_train))\n",
    "print(\"X val shape:\", len(X_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_train = ImageDataGenerator(preprocessing_function = lambda x: x/255.,\n",
    "                             width_shift_range=4,  # randomly shift images horizontally\n",
    "                             height_shift_range=4,  # randomly shift images vertically \n",
    "                             horizontal_flip=True,  # randomly flip images\n",
    "                             vertical_flip=True)  # randomly flip images\n",
    "datagen_validation = ImageDataGenerator(preprocessing_function=lambda x: x/255.)\n",
    "datagen_test = ImageDataGenerator(preprocessing_function = lambda x: x/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12611 validated image filenames.\n",
      "Found 1425 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_truth = pd.read_csv('./data.nosync/train/train.csv')\n",
    "validation_truth = pd.read_csv('./data.nosync/validation/Validation Dataset.csv')\n",
    "train_truth['boneage'] = train_truth['boneage']/200\n",
    "train_truth['id'] = train_truth['id'].astype(str) + '.png'\n",
    "validation_truth['Bone Age (months)'] = validation_truth['Bone Age (months)']/200\n",
    "train_generator = datagen_train.flow_from_dataframe(dataframe=train_truth, directory='./data.nosync/train/images', \n",
    "                                                    x_col=\"id\",\n",
    "                                                    y_col=\"boneage\",  \n",
    "                                                    class_mode=\"other\", target_size=(IMG_SHAPE[0], IMG_SHAPE[1]), \n",
    "                                                    batch_size=batch_size)\n",
    "validation_truth['Image ID'] = validation_truth['Image ID'].astype(str) + '.png'\n",
    "validation_generator = datagen_validation.flow_from_dataframe(dataframe=validation_truth, directory='./data.nosync/validation/images', \n",
    "                                                              x_col=\"Image ID\", \n",
    "                                                              y_col=\"Bone Age (months)\", \n",
    "                                                              class_mode=\"other\", target_size=(IMG_SHAPE[0], IMG_SHAPE[1]), \n",
    "                                                              batch_size=batch_size)\n",
    "batch_size = 32\n",
    "train_steps = int(np.ceil( train_generator.n / batch_size))\n",
    "validation_steps = int(np.ceil(validation_generator.n / batch_size))\n",
    "# plt.imshow(train_generator.next()[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second branch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_btw_bones1 = np.random.uniform(1, 100, size=len(sex_train))\n",
    "dist_btw_bones2 = np.random.uniform(1, 100, size=len(sex_train))\n",
    "dist_btw_bones3 = np.random.uniform(1, 100, size=len(sex_train))\n",
    "dist_btw_bones4 = np.random.uniform(1, 100, size=len(sex_train))\n",
    "\n",
    "dist_btw_bones1 = np.random.normal(age_train, 1, size=len(sex_train))\n",
    "dist_btw_bones2 = np.random.normal(age_train, 1, size=len(sex_train))\n",
    "dist_btw_bones3 = np.random.normal(age_train, 1, size=len(sex_train))\n",
    "dist_btw_bones4 = np.random.normal(age_train, 1, size=len(sex_train))\n",
    "\n",
    "second_branch_df_train = pd.DataFrame(list(zip(sex_train, dist_btw_bones1, dist_btw_bones2, dist_btw_bones3, dist_btw_bones4)),\n",
    "                            columns =['sex', 'd1', 'd2', 'd3', 'd4'])\n",
    "second_branch_df_train.sex = second_branch_df_train.sex.replace({True: 1., False: 0.})\n",
    "\n",
    "dist_btw_bones1_val = np.random.normal(age_validation, 10, size=len(sex_validation))\n",
    "dist_btw_bones2_val = np.random.normal(age_validation, 10, size=len(sex_validation))\n",
    "dist_btw_bones3_val = np.random.normal(age_validation, 10, size=len(sex_validation))\n",
    "dist_btw_bones4_val = np.random.normal(age_validation, 10, size=len(sex_validation))\n",
    "second_branch_df_val = pd.DataFrame(list(zip(sex_validation, dist_btw_bones1_val, dist_btw_bones2_val, dist_btw_bones3_val, dist_btw_bones4_val)),\n",
    "                            columns =['sex', 'd1', 'd2', 'd3', 'd4'])\n",
    "second_branch_df_val.sex = second_branch_df_val.sex.replace({True: 1, False: 0})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try if the second branch works by itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_dataset_branch2(second_branch_df, ages, batch_size, shuffle, cache_file=None):\n",
    "\n",
    "#     # Create a Dataset object\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((second_branch_df, ages))\n",
    "\n",
    "#     # Cache dataset\n",
    "#     if cache_file:\n",
    "#         dataset = dataset.cache(cache_file)\n",
    "\n",
    "\n",
    "#     # Shuffle\n",
    "#     if shuffle:\n",
    "#         dataset = dataset.shuffle(len(second_branch_df))\n",
    "\n",
    "#     # Repeat the dataset indefinitely\n",
    "#     dataset = dataset.repeat()\n",
    "\n",
    "#     # Batch\n",
    "#     dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "#     # Prefetch\n",
    "#     dataset = dataset.prefetch(buffer_size=1)\n",
    "\n",
    "#     return dataset\n",
    "\n",
    "# batch_size = 32\n",
    "# train_dataset_branch_2 = create_dataset_branch2(second_branch_df = second_branch_df_train, \n",
    "#                     ages = age_train, \n",
    "#                     batch_size = batch_size, \n",
    "#                     shuffle = False )  \n",
    "\n",
    "# validation_dataset_branch_2 = create_dataset_branch2(second_branch_df = second_branch_df_val, \n",
    "#                     ages = age_validation, \n",
    "#                     batch_size = batch_size, \n",
    "#                     shuffle = False ) \n",
    "\n",
    "# def dense_branch_2(X_input):\n",
    "#     X = Dense(64, activation='relu', name='first_dense_branch_2')(X_input)\n",
    "#     X = Dense(32, activation='relu', name='second_dense_branch_2')(X)\n",
    "#     X = Flatten()(X)\n",
    "#     # max_pooling o global_pooling (valore unico) [regolarizza vs overfitting] o dense più piccolo - > dense + pooling\n",
    "#     return X\n",
    "\n",
    "\n",
    "# def model_assembly_example(input_shape_dataset):\n",
    "#     # branch 2\n",
    "#     X_input_branch2 = Input(input_shape_dataset)\n",
    "#     branch2 = dense_branch_2(X_input_branch2)\n",
    "#     X = Dense(1, activation = 'relu', name='final')(branch2)\n",
    "\n",
    "\n",
    "#     # Create model\n",
    "#     model = Model(inputs = X_input_branch2, outputs = X, name='branch2_attempt')\n",
    "#     return model\n",
    "\n",
    "# train_steps = int(np.ceil(len(sex_train) / batch_size))\n",
    "# val_steps = int(np.ceil(len(sex_validation) / batch_size))\n",
    "\n",
    "# model = model_assembly_example(input_shape_dataset=5)\n",
    "\n",
    "# model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=tf.keras.metrics.mean_squared_error)\n",
    "\n",
    "# # Create a callback for early stopping \n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# history = model.fit(train_dataset_branch_2, \n",
    "#                     validation_data = validation_dataset_branch_2, \n",
    "#                     epochs=100, \n",
    "#                     steps_per_epoch=train_steps,\n",
    "#                     validation_steps=val_steps,\n",
    "#                     callbacks=[callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Inceptionv4(X_input): #  (input_shape):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception-v4 architecture\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    # # Define the input as a tensor with shape input_shape (1 line)\n",
    "    # X_input = Input(input_shape)\n",
    "\n",
    "    # Call the above functions for the stem, inception-a, reduction-a, inception-b, reduction-b and inception-c blocks\n",
    "    X = stem_block(X_input)\n",
    "\n",
    "    # Four Inception A blocks\n",
    "    X = inception_a_block(X, 'a1')\n",
    "    X = inception_a_block(X, 'a2')\n",
    "    X = inception_a_block(X, 'a3')\n",
    "    X = inception_a_block(X, 'a4')\n",
    "\n",
    "    # Reduction A block\n",
    "    X = reduction_a_block(X)\n",
    "\n",
    "    # Seven Inception B blocks\n",
    "    X = inception_b_block(X, 'b1')\n",
    "    X = inception_b_block(X, 'b2')\n",
    "    X = inception_b_block(X, 'b3')\n",
    "    X = inception_b_block(X, 'b4')\n",
    "    X = inception_b_block(X, 'b5')\n",
    "    X = inception_b_block(X, 'b6')\n",
    "    X = inception_b_block(X, 'b7')\n",
    "\n",
    "    # Reduction B block\n",
    "    X = reduction_b_block(X)\n",
    "\n",
    "    # Three Inception C blocks\n",
    "    X = inception_c_block(X, 'c1')\n",
    "    X = inception_c_block(X, 'c2')\n",
    "    X = inception_c_block(X, 'c3')\n",
    "\n",
    "    # AVGPOOL (1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
    "    kernel_pooling = X.get_shape()[1:3]\n",
    "    X = AveragePooling2D(kernel_pooling, name='avg_pool')(X)\n",
    "    X = Flatten()(X)\n",
    "\n",
    "    # Dropout\n",
    "    X = Dropout(rate = 0.2)(X)\n",
    "\n",
    "    # Output layer\n",
    "    # X = Dense(1, activation='relu', name='fc')(X)\n",
    "    \n",
    "    \n",
    "    # # Create model\n",
    "    # model = Model(inputs = X_input, outputs = X, name='Inceptionv4')\n",
    "\n",
    "    # return model\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_branch_2(X_input):\n",
    "    X = Dense(64, activation='relu', name='first_dense_branch_2')(X_input)\n",
    "    X = Dense(32, activation='relu', name='second_dense_branch_2')(X)\n",
    "    X = Flatten()(X)\n",
    "    # max_pooling o global_pooling (valore unico) [regolarizza vs overfitting] o dense più piccolo - > dense + pooling\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IF YOU WANT TO LOAD THE DATA BEFORE HAND \n",
    "# full_df_train = second_branch_df_train\n",
    "# full_df_train['img_path'] = X_train\n",
    "\n",
    "# full_df_val = second_branch_df_val\n",
    "# full_df_val['img_path'] = X_validation\n",
    "\n",
    "# full_df_train['img'] = full_df_train.apply(lambda row: load_image(row['img_path'], IMG_SHAPE), axis = 1) \n",
    "# full_df_train = full_df_train.drop(['img_path'], axis=1)\n",
    "\n",
    "# full_df_val['img'] = full_df_val.apply(lambda row: load_image(row['img_path'], IMG_SHAPE), axis = 1) \n",
    "# full_df_val = full_df_val.drop(['img_path'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_image(path_df, age):\n",
    "\n",
    "#     num_row = IMG_SHAPE[1]\n",
    "#     num_col = IMG_SHAPE[0]\n",
    "\n",
    "#     img_path, df = path_df\n",
    "#     img_path = str(img_path)\n",
    "#     if isinstance(img_path, bytes):\n",
    "#         img_path = img_path.decode()\n",
    "    \n",
    "#     img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "#     img = np.array(cv2.resize(img, (num_row, num_col)), dtype='float32')\n",
    "\n",
    "#     # Normalize image\n",
    "#     img = img/255.0\n",
    "#     #return img, df\n",
    "#     return (img, df), age\n",
    "\n",
    "def process_image_2(path_df, label):\n",
    "    # Desired size\n",
    "    # num_row = IMG_SHAPE[1]\n",
    "    # num_col = IMG_SHAPE[0]\n",
    "    size = IMG_SHAPE[1]\n",
    "    path, df = path_df\n",
    "\n",
    "    # Get the image\n",
    "    img = tf.io.read_file(path)\n",
    "    # Decode the PNG\n",
    "    img = tf.image.decode_png(img)\n",
    "    # Resize image\n",
    "    img = tf.image.resize(img, (size, size))\n",
    "    # Reshape image (this is not necessary but I do it so that I don't need to be modifying the shape in the input layer)\n",
    "    #img = tf.reshape(img, [size, size, 3])\n",
    "    # Cast image to float32\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    # Normalize image\n",
    "    img = img/255.0\n",
    "\n",
    "    return (img, df), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_images(img_paths, second_branch_df, ages, batch_size, shuffle, cache_file=None):\n",
    "\n",
    "    # Create a Dataset object\n",
    "    second_branch_df = second_branch_df.astype('float32')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((img_paths, second_branch_df), ages)).map(process_image_2)\n",
    "\n",
    "    # Cache dataset\n",
    "    if cache_file:\n",
    "        dataset = dataset.cache(cache_file)\n",
    "\n",
    "    # Shuffle\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(img_paths))\n",
    "\n",
    "    # Repeat the dataset indefinitely\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "    # Batch\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "    # Prefetch\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = create_dataset_images(  img_paths = X_train, \n",
    "                                        second_branch_df = second_branch_df_train,\n",
    "                                        ages = age_train_df, \n",
    "                                        batch_size = batch_size, \n",
    "                                        shuffle = False,\n",
    "                                        cache_file='cache_train' )  \n",
    "\n",
    "validation_dataset = create_dataset_images(img_paths = X_validation, \n",
    "                                            second_branch_df = second_branch_df_val,\n",
    "                                            ages = age_validation_df, \n",
    "                                            batch_size = batch_size, \n",
    "                                            shuffle = False ,\n",
    "                                        cache_file='cache_val') \n",
    "\n",
    "train_steps = int(np.ceil(len(X_train) / batch_size))\n",
    "validation_steps = int(np.ceil(len(X_validation) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_assembly(input_shape_img, input_shape_dataset):\n",
    "\n",
    "    # Branch 1\n",
    "    X_input_branch1 = Input(input_shape_img)\n",
    "    print(input_shape_img)\n",
    "    branch1 = Inceptionv4(X_input_branch1)\n",
    "\n",
    "    # # Branch 2\n",
    "    X_input_branch2 = Input(input_shape_dataset)\n",
    "    branch2 = dense_branch_2(X_input_branch2)\n",
    "    \n",
    "    # # Concatenate branch1 and branch2\n",
    "    X = tf.concat(values=[branch1, branch2], axis=1)\n",
    "    X = Dense(1000, activation='relu', name = 'final_dense_1')(X)\n",
    "    X = Dense(1000, activation='relu', name = 'final_dense_2')(X)\n",
    "    X_out = Dense(1, activation = 'linear', name='final')(X)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs = (X_input_branch1, X_input_branch2) , outputs = X_out, name='model0') #  X_input_branch2\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299, 299, 1)\n"
     ]
    }
   ],
   "source": [
    "model = model_assembly(input_shape_img = IMG_SHAPE, input_shape_dataset = 5)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=tf.keras.metrics.mean_squared_error)\n",
    "\n",
    "# Create a callback for early stopping \n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14/16 [=========================>....] - ETA: 19s - loss: 7.3133 - mean_squared_error: 7.3133"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 11:31:46.838983: W tensorflow/core/kernels/data/cache_dataset_ops.cc:296] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - ETA: 0s - loss: 6.4172 - mean_squared_error: 6.4172 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 11:32:54.456369: W tensorflow/core/kernels/data/cache_dataset_ops.cc:296] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 217s 13s/step - loss: 6.4172 - mean_squared_error: 6.4172 - val_loss: 0.9809 - val_mean_squared_error: 0.9809\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 195s 12s/step - loss: 0.0686 - mean_squared_error: 0.0686 - val_loss: 0.4297 - val_mean_squared_error: 0.4297\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 190s 12s/step - loss: 0.0533 - mean_squared_error: 0.0533 - val_loss: 0.2744 - val_mean_squared_error: 0.2744\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 191s 12s/step - loss: 0.0499 - mean_squared_error: 0.0499 - val_loss: 0.1460 - val_mean_squared_error: 0.1460\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 195s 12s/step - loss: 0.0432 - mean_squared_error: 0.0432 - val_loss: 0.1222 - val_mean_squared_error: 0.1222\n",
      "Epoch 6/10\n",
      " 9/16 [===============>..............] - ETA: 1:05 - loss: 0.0406 - mean_squared_error: 0.0406"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Fit the model on batches with real-time data augmentation:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_dataset,\n\u001b[1;32m      3\u001b[0m                     validation_data \u001b[39m=\u001b[39;49m validation_dataset, \n\u001b[1;32m      4\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \n\u001b[1;32m      5\u001b[0m                     steps_per_epoch\u001b[39m=\u001b[39;49mtrain_steps,\n\u001b[1;32m      6\u001b[0m                     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m      7\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[callback])\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/hda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/hda/lib/python3.10/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/hda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/hda/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/hda/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/hda/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/hda/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/hda/lib/python3.10/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/hda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the model on batches with real-time data augmentation:\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data = validation_dataset, \n",
    "                    epochs=10, \n",
    "                    steps_per_epoch=train_steps,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=[callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc7584d7085398ca59d1bc7417060e07bb5363bf245a5ad688f6fa83be570210"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
